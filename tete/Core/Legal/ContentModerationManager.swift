//======================================================================
// MARK: - ContentModerationManager.swift
// Purpose: Manager class for system operations (ContentModerationManagerã®ã‚·ã‚¹ãƒ†ãƒ æ“ä½œç®¡ç†ã‚¯ãƒ©ã‚¹)
// Path: tete/Core/Legal/ContentModerationManager.swift
//======================================================================
//
//  ContentModerationManager.swift
//  tete
//
//  ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨å¹´é½¢åˆ¶é™ã‚·ã‚¹ãƒ†ãƒ 
//

import Foundation
import SwiftUI
@preconcurrency import Vision
import CoreML

// MARK: - Enums
enum RestrictionReason: String, Codable {
    case inappropriate = "inappropriate"
    case ageRestricted = "age_restricted"
    case violatesGuidelines = "violates_guidelines"
    case reported = "reported"
    case temporaryBan = "temporary_ban"
    case permanentBan = "permanent_ban"
}

// MARK: - Content Moderation Manager
@MainActor
final class ContentModerationManager: ObservableObject {
    
    static let shared = ContentModerationManager()
    
    // MARK: - Published Properties
    @Published var moderationQueue: [ModerationItem] = []
    @Published var bannedUsers: Set<String> = []
    @Published var restrictedContent: [String: RestrictionReason] = [:]
    
    // MARK: - Private Properties
    private let imageAnalyzer = VNImageRequestHandler(cgImage: UIImage().cgImage!)
    private var contentClassifier: VNCoreMLModel?
    
    // MARK: - Initialization
    private init() {
        setupContentClassifier()
        loadModerationData()
    }
    
    // MARK: - Content Analysis
    
    /// ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ†æ
    @MainActor
    func analyzeImage(_ image: UIImage) async -> ContentAnalysisResult {
        return await withCheckedContinuation { continuation in
            guard let cgImage = image.cgImage else {
                continuation.resume(returning: ContentAnalysisResult(
                    isAppropriate: false,
                    confidence: 0.0,
                    flags: [.technicalError],
                    ageRating: .mature
                ))
                return
            }
            
            let handler = VNImageRequestHandler(cgImage: cgImage)
            var analysisResult = ContentAnalysisResult(
                isAppropriate: true,
                confidence: 1.0,
                flags: [],
                ageRating: .general
            )
            
            // 1. æˆäººå‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡º
            let adultContentRequest = VNClassifyImageRequest { request, error in
                if let results = request.results as? [VNClassificationObservation] {
                    for result in results {
                        if result.identifier.contains("explicit") && result.confidence > 0.7 {
                            analysisResult.flags.append(.explicitContent)
                            analysisResult.ageRating = .mature
                            analysisResult.isAppropriate = false
                        }
                    }
                }
            }
            
            // 2. ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºï¼ˆç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            let textRequest = VNRecognizeTextRequest { request, error in
                if let results = request.results as? [VNRecognizedTextObservation] {
                    for result in results {
                        if let text = result.topCandidates(1).first?.string {
                            let textAnalysis = self.analyzeTextContent(text)
                            if !textAnalysis.isAppropriate {
                                analysisResult.flags.append(contentsOf: textAnalysis.flags)
                                analysisResult.isAppropriate = false
                            }
                        }
                    }
                }
            }
            
            // 3. é¡”æ¤œå‡ºï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ï¼‰
            let faceRequest = VNDetectFaceRectanglesRequest { request, error in
                if let results = request.results as? [VNFaceObservation], results.count > 5 {
                    analysisResult.flags.append(.multiplePersons)
                    analysisResult.requiresReview = true
                }
            }
            
            // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            Task { @MainActor in
                do {
                    try handler.perform([adultContentRequest, textRequest, faceRequest])
                    
                    // æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹è¿½åŠ åˆ†æ
                    if let classifier = self.contentClassifier {
                        let classificationRequest = VNCoreMLRequest(model: classifier) { request, error in
                            if let results = request.results as? [VNClassificationObservation] {
                                for result in results {
                                    if result.identifier == "inappropriate" && result.confidence > 0.8 {
                                        analysisResult.flags.append(.inappropriateContent)
                                        analysisResult.isAppropriate = false
                                    }
                                }
                            }
                        }
                        try handler.perform([classificationRequest])
                    }
                    
                    // æœ€çµ‚çš„ãªä¿¡é ¼åº¦ã‚’è¨ˆç®—
                    analysisResult.confidence = self.calculateConfidence(for: analysisResult)
                    
                    continuation.resume(returning: analysisResult)
                    
                } catch {
                    analysisResult.flags.append(.technicalError)
                    analysisResult.isAppropriate = false
                    continuation.resume(returning: analysisResult)
                }
            }
        }
    }
    
    /// ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ†æ
    func analyzeTextContent(_ text: String) -> ContentAnalysisResult {
        var flags: [ContentFlag] = []
        var isAppropriate = true
        var ageRating: AgeRating = .general
        
        let lowercaseText = text.lowercased()
        
        // 1. ä¸é©åˆ‡ãªè¨€è‘‰ã®æ¤œå‡º
        let inappropriateWords = getInappropriateWords()
        for word in inappropriateWords {
            if lowercaseText.contains(word.lowercased()) {
                flags.append(.inappropriateLanguage)
                isAppropriate = false
                ageRating = .mature
                break
            }
        }
        
        // 2. ãƒ˜ã‚¤ãƒˆã‚¹ãƒ”ãƒ¼ãƒã®æ¤œå‡º
        let hateWords = getHateWords()
        for word in hateWords {
            if lowercaseText.contains(word.lowercased()) {
                flags.append(.hateSpeech)
                isAppropriate = false
                ageRating = .mature
                break
            }
        }
        
        // 3. ã‚¹ãƒ‘ãƒ ã®æ¤œå‡º
        if isSpamContent(text) {
            flags.append(.spam)
            isAppropriate = false
        }
        
        // 4. å€‹äººæƒ…å ±ã®æ¤œå‡º
        if containsPersonalInfo(text) {
            flags.append(.personalInformation)
            ageRating = .teen
        }
        
        return ContentAnalysisResult(
            isAppropriate: isAppropriate,
            confidence: 0.9,
            flags: flags,
            ageRating: ageRating
        )
    }
    
    // MARK: - Age Verification
    
    /// å¹´é½¢åˆ¶é™ã®ç¢ºèª
    func checkAgeRestriction(content: ContentAnalysisResult, userAge: Int?) -> Bool {
        guard let userAge = userAge else {
            // å¹´é½¢ä¸æ˜ã®å ´åˆã¯æœ€ã‚‚å³ã—ã„åˆ¶é™ã‚’é©ç”¨
            return content.ageRating == .general
        }
        
        switch content.ageRating {
        case .general:
            return true
        case .teen:
            return userAge >= 13
        case .mature:
            return userAge >= 18
        }
    }
    
    /// ä¿è­·è€…ã«ã‚ˆã‚‹åˆ¶é™è¨­å®š
    func applyParentalControls(_ controls: ParentalControls, userId: String) {
        let controlsData = try? JSONEncoder().encode(controls)
        UserDefaults.standard.set(controlsData, forKey: "ParentalControls_\(userId)")
        
        print("ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Parental controls applied for user: \(userId)")
    }
    
    /// ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ã®é©ç”¨
    func shouldFilterContent(_ content: ContentAnalysisResult, for userId: String) -> Bool {
        // ä¿è­·è€…ã«ã‚ˆã‚‹åˆ¶é™ã‚’ç¢ºèª
        if let controlsData = UserDefaults.standard.data(forKey: "ParentalControls_\(userId)"),
           let controls = try? JSONDecoder().decode(ParentalControls.self, from: controlsData) {
            
            // åˆ¶é™ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãåˆ¤å®š
            switch controls.restrictionLevel {
            case .none:
                return false
            case .light:
                return content.flags.contains(.explicitContent) || content.flags.contains(.hateSpeech)
            case .moderate:
                return !content.isAppropriate || content.ageRating == .mature
            case .strict:
                return content.ageRating != .general
            }
        }
        
        // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æˆäººå‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        return content.flags.contains(.explicitContent)
    }
    
    // MARK: - Moderation Actions
    
    /// ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å ±å‘Šå‡¦ç†
    func reportContent(_ contentId: String, reason: ReportReason, reporterId: String) {
        let report = ContentReport(
            id: UUID().uuidString,
            contentId: contentId,
            reporterId: reporterId,
            reason: reason,
            timestamp: Date(),
            status: .pending
        )
        
        addToModerationQueue(report)
        
        // ç·Šæ€¥åº¦ã®é«˜ã„å ±å‘Šã¯å³åº§ã«å‡¦ç†
        if reason == .illegalContent || reason == .harmToMinors {
            Task {
                await processUrgentReport(report)
            }
        }
        
        print("ğŸš¨ Content reported: \(contentId) for \(reason)")
    }
    
    /// è‡ªå‹•ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    func automaticModeration(_ content: PostContent) async -> ModerationDecision {
        // ç”»åƒåˆ†æ
        var imageAnalysis: ContentAnalysisResult?
        if let image = content.image {
            imageAnalysis = await analyzeImage(image)
        }
        
        // ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
        let textAnalysis = analyzeTextContent(content.caption)
        
        // ç·åˆåˆ¤å®š
        let overallAppropriate = (imageAnalysis?.isAppropriate ?? true) && textAnalysis.isAppropriate
        
        if !overallAppropriate {
            // è‡ªå‹•å‰Šé™¤ã®åŸºæº–
            let shouldAutoRemove = hasAutoRemoveFlags(imageAnalysis, textAnalysis)
            
            if shouldAutoRemove {
                return .remove(reason: "Automatic moderation: inappropriate content")
            } else {
                return .requiresReview(priority: .high)
            }
        }
        
        return .approve
    }
    
    /// ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è­¦å‘Šãƒ»åˆ¶è£
    func issueWarning(to userId: String, reason: String) {
        let warning = UserWarning(
            userId: userId,
            reason: reason,
            timestamp: Date(),
            severity: .warning
        )
        
        saveUserWarning(warning)
        
        // è­¦å‘Šå±¥æ­´ã®ç¢ºèª
        let warningCount = getUserWarningCount(userId)
        
        if warningCount >= 3 {
            // 3å›è­¦å‘Šã§ä¸€æ™‚åœæ­¢
            suspendUser(userId, duration: .days(7), reason: "Multiple violations")
        }
        
        print("âš ï¸ Warning issued to user: \(userId)")
    }
    
    /// ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸€æ™‚åœæ­¢
    func suspendUser(_ userId: String, duration: SuspensionDuration, reason: String) {
        let suspension = UserSuspension(
            userId: userId,
            reason: reason,
            startDate: Date(),
            endDate: Date().addingTimeInterval(duration.timeInterval),
            isActive: true
        )
        
        saveUserSuspension(suspension)
        print("ğŸš« User suspended: \(userId) for \(duration)")
    }
    
    /// ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ°¸ä¹…åœæ­¢
    func banUser(_ userId: String, reason: String) {
        bannedUsers.insert(userId)
        
        let ban = UserBan(
            userId: userId,
            reason: reason,
            timestamp: Date(),
            isActive: true
        )
        
        saveUserBan(ban)
        print("ğŸ”¨ User banned: \(userId)")
    }
    
    // MARK: - AI-Powered Moderation
    
    /// æ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜åº¦ãªãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    func advancedContentAnalysis(_ content: PostContent) async -> AdvancedAnalysisResult {
        // è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸåˆ†æ
        var analysisResults: [String: Any] = [:]
        
        // 1. æ„Ÿæƒ…åˆ†æ
        let sentimentScore = analyzeSentiment(content.caption)
        analysisResults["sentiment"] = sentimentScore
        
        // 2. ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡
        let topics = classifyTopics(content.caption)
        analysisResults["topics"] = topics
        
        // 3. ç”»åƒã®æ–‡è„ˆç†è§£
        if let image = content.image {
            let imageContext = await analyzeImageContext(image)
            analysisResults["imageContext"] = imageContext
        }
        
        // 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        let userPattern = analyzeUserBehavior(content.authorId)
        analysisResults["userPattern"] = userPattern
        
        return AdvancedAnalysisResult(
            confidence: 0.85,
            riskScore: calculateRiskScore(analysisResults),
            recommendations: generateRecommendations(analysisResults),
            details: analysisResults
        )
    }
    
    // MARK: - Community Guidelines
    
    /// ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³é•åã®æ¤œå‡º
    func checkCommunityGuidelines(_ content: PostContent) -> [GuidelineViolation] {
        var violations: [GuidelineViolation] = []
        
        // 1. ã‚¹ãƒ‘ãƒ ãƒ»å®£ä¼
        if isCommercialSpam(content.caption) {
            violations.append(.commercialSpam)
        }
        
        // 2. è‘—ä½œæ¨©ä¾µå®³ã®å¯èƒ½æ€§
        if containsCopyrightedContent(content) {
            violations.append(.copyrightInfringement)
        }
        
        // 3. æš´åŠ›çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if containsViolentContent(content) {
            violations.append(.violence)
        }
        
        // 4. è‡ªå‚·è¡Œç‚ºã®ä¿ƒé€²
        if promotesSelfHarm(content.caption) {
            violations.append(.selfHarmPromotion)
        }
        
        return violations
    }
    
    // MARK: - Private Methods
    
    private func setupContentClassifier() {
        // Core MLãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ï¼‰
        // contentClassifier = try? VNCoreMLModel(for: ContentClassifierModel().model)
    }
    
    private func loadModerationData() {
        // éå»ã®ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        if let bannedData = UserDefaults.standard.data(forKey: "BannedUsers"),
           let banned = try? JSONDecoder().decode(Set<String>.self, from: bannedData) {
            bannedUsers = banned
        }
    }
    
    private func getInappropriateWords() -> [String] {
        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å–å¾—
        return ["badword1", "badword2", "inappropriate"]
    }
    
    private func getHateWords() -> [String] {
        // ãƒ˜ã‚¤ãƒˆã‚¹ãƒ”ãƒ¼ãƒé–¢é€£ã®å˜èªãƒªã‚¹ãƒˆ
        return ["hate1", "hate2", "discriminatory"]
    }
    
    private func isSpamContent(_ text: String) -> Bool {
        // ã‚¹ãƒ‘ãƒ æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯
        let spamIndicators = ["click here", "free money", "limited time"]
        return spamIndicators.contains { text.lowercased().contains($0) }
    }
    
    private func containsPersonalInfo(_ text: String) -> Bool {
        // å€‹äººæƒ…å ±æ¤œå‡ºï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€é›»è©±ç•ªå·ãªã©ï¼‰
        let emailRegex = try? NSRegularExpression(pattern: "[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}", options: .caseInsensitive)
        return emailRegex?.firstMatch(in: text, range: NSRange(location: 0, length: text.count)) != nil
    }
    
    private func calculateConfidence(for result: ContentAnalysisResult) -> Double {
        var confidence = 1.0
        
        // ãƒ•ãƒ©ã‚°ã®æ•°ã«åŸºã¥ã„ã¦ä¿¡é ¼åº¦ã‚’èª¿æ•´
        confidence -= Double(result.flags.count) * 0.1
        
        return max(0.1, confidence)
    }
    
    private func addToModerationQueue(_ report: ContentReport) {
        let item = ModerationItem(
            id: report.id,
            type: .userReport,
            contentId: report.contentId,
            priority: getPriority(for: report.reason),
            timestamp: report.timestamp
        )
        
        moderationQueue.append(item)
        moderationQueue.sort { $0.priority.rawValue < $1.priority.rawValue }
    }
    
    private func getPriority(for reason: ReportReason) -> ModerationPriority {
        switch reason {
        case .illegalContent, .harmToMinors:
            return .urgent
        case .hateSpeech, .harassment:
            return .high
        case .spam, .inappropriateContent:
            return .medium
        default:
            return .low
        }
    }
    
    private func processUrgentReport(_ report: ContentReport) async {
        // ç·Šæ€¥å ±å‘Šã®å³åº§å‡¦ç†
        print("ğŸš¨ Processing urgent report: \(report.id)")
    }
    
    private func hasAutoRemoveFlags(_ imageAnalysis: ContentAnalysisResult?, _ textAnalysis: ContentAnalysisResult) -> Bool {
        let dangerousFlags: [ContentFlag] = [.explicitContent, .hateSpeech, .harmToMinors]
        
        let imageFlags = imageAnalysis?.flags ?? []
        let textFlags = textAnalysis.flags
        
        return dangerousFlags.contains { imageFlags.contains($0) || textFlags.contains($0) }
    }
    
    // ãã®ä»–ã®ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå®Ÿè£…çœç•¥ï¼‰
    private func saveUserWarning(_ warning: UserWarning) { /* å®Ÿè£… */ }
    private func getUserWarningCount(_ userId: String) -> Int { return 0 }
    private func saveUserSuspension(_ suspension: UserSuspension) { /* å®Ÿè£… */ }
    private func saveUserBan(_ ban: UserBan) { /* å®Ÿè£… */ }
    private func analyzeSentiment(_ text: String) -> Double { return 0.0 }
    private func classifyTopics(_ text: String) -> [String] { return [] }
    private func analyzeImageContext(_ image: UIImage) async -> [String: Any] { return [:] }
    private func analyzeUserBehavior(_ userId: String) -> [String: Any] { return [:] }
    private func calculateRiskScore(_ results: [String: Any]) -> Double { return 0.0 }
    private func generateRecommendations(_ results: [String: Any]) -> [String] { return [] }
    private func isCommercialSpam(_ text: String) -> Bool { return false }
    private func containsCopyrightedContent(_ content: PostContent) -> Bool { return false }
    private func containsViolentContent(_ content: PostContent) -> Bool { return false }
    private func promotesSelfHarm(_ text: String) -> Bool { return false }
}

// MARK: - Supporting Types

struct ContentAnalysisResult {
    var isAppropriate: Bool
    var confidence: Double
    var flags: [ContentFlag]
    var ageRating: AgeRating
    var requiresReview: Bool = false
}

enum ContentFlag {
    case explicitContent
    case inappropriateLanguage
    case hateSpeech
    case spam
    case personalInformation
    case violence
    case inappropriateContent
    case multiplePersons
    case harmToMinors
    case technicalError
}

enum AgeRating {
    case general    // å…¨å¹´é½¢
    case teen      // 13æ­³ä»¥ä¸Š
    case mature    // 18æ­³ä»¥ä¸Š
}

struct ParentalControls: Codable {
    let restrictionLevel: RestrictionLevel
    let allowedCategories: [ContentCategory]
    let timeRestrictions: TimeRestrictions?
    let blockedUsers: [String]
    
    enum RestrictionLevel: String, Codable, CaseIterable {
        case none = "none"
        case light = "light"
        case moderate = "moderate"
        case strict = "strict"
    }
}

enum ContentCategory: String, Codable, CaseIterable {
    case food = "food"
    case travel = "travel"
    case nature = "nature"
    case art = "art"
    case lifestyle = "lifestyle"
}

struct TimeRestrictions: Codable {
    let dailyLimit: TimeInterval // 1æ—¥ã®åˆ©ç”¨æ™‚é–“åˆ¶é™
    let allowedHours: [Int] // åˆ©ç”¨å¯èƒ½æ™‚é–“å¸¯
    let blockNighttime: Bool // å¤œé–“åˆ©ç”¨åˆ¶é™
}

enum ReportReason: String, Codable, CaseIterable {
    case inappropriateContent = "inappropriate"
    case spam = "spam"
    case harassment = "harassment"
    case hateSpeech = "hate_speech"
    case violence = "violence"
    case illegalContent = "illegal"
    case copyrightInfringement = "copyright"
    case harmToMinors = "harm_minors"
    case personalInformation = "personal_info"
    case other = "other"
}

struct ContentReport: Codable {
    let id: String
    let contentId: String
    let reporterId: String
    let reason: ReportReason
    let timestamp: Date
    var status: ReportStatus
    
    enum ReportStatus: String, Codable {
        case pending = "pending"
        case reviewed = "reviewed"
        case resolved = "resolved"
        case dismissed = "dismissed"
    }
}

struct ModerationItem {
    let id: String
    let type: ModerationType
    let contentId: String
    let priority: ModerationPriority
    let timestamp: Date
    
    enum ModerationType {
        case automaticFlag
        case userReport
        case algorithmicDetection
    }
}

enum ModerationPriority: Int, CaseIterable {
    case urgent = 1
    case high = 2
    case medium = 3
    case low = 4
}

enum ModerationDecision {
    case approve
    case remove(reason: String)
    case requiresReview(priority: ModerationPriority)
    case shadowBan
}

struct UserWarning: Codable {
    let userId: String
    let reason: String
    let timestamp: Date
    let severity: Severity
    
    enum Severity: String, Codable {
        case warning = "warning"
        case serious = "serious"
        case final = "final"
    }
}

struct UserSuspension: Codable {
    let userId: String
    let reason: String
    let startDate: Date
    let endDate: Date
    var isActive: Bool
}

struct UserBan: Codable {
    let userId: String
    let reason: String
    let timestamp: Date
    var isActive: Bool
}

enum SuspensionDuration {
    case hours(Int)
    case days(Int)
    case weeks(Int)
    case permanent
    
    var timeInterval: TimeInterval {
        switch self {
        case .hours(let h): return TimeInterval(h * 3600)
        case .days(let d): return TimeInterval(d * 24 * 3600)
        case .weeks(let w): return TimeInterval(w * 7 * 24 * 3600)
        case .permanent: return TimeInterval.greatestFiniteMagnitude
        }
    }
}

struct PostContent {
    let authorId: String
    let caption: String
    let image: UIImage?
    let location: String?
    let hashtags: [String]
}

struct AdvancedAnalysisResult {
    let confidence: Double
    let riskScore: Double
    let recommendations: [String]
    let details: [String: Any]
}

enum GuidelineViolation {
    case commercialSpam
    case copyrightInfringement
    case violence
    case selfHarmPromotion
    case harassment
    case impersonation
}